# Llama Models Documentation

## Note of Deprecation

Thank you for developing with Llama models. As part of the Llama 3.1 release, we’ve consolidated GitHub repositories and added new ones as we’ve expanded Llama’s functionality into an end-to-end Llama Stack. Please use the following repositories going forward:

- [llama-models](https://github.com/meta-llama/llama-models) - Central repo for foundation models, including basic utilities, model cards, license, and use policies.
- [PurpleLlama](https://github.com/meta-llama/PurpleLlama) - Key component of Llama Stack focusing on safety risks and inference time mitigations.
- [llama-toolchain](https://github.com/meta-llama/llama-toolchain) - Interfaces and canonical implementations for model development, including inference, fine-tuning, safety shields, and synthetic data generation.
- [llama-agentic-system](https://github.com/meta-llama/llama-agentic-system) - End-to-end standalone Llama Stack system, including an opinionated interface for creating agentic applications.
- [llama-recipes](https://github.com/meta-llama/llama-recipes) - Community-driven scripts and integrations.

If you have any questions, please file an issue on any of the above repos, and we will do our best to respond in a timely manner.

Thank you!

## (Deprecated) Llama 2

Llama 2 is now accessible to individuals, creators, researchers, and businesses of all sizes to experiment, innovate, and scale ideas responsibly. This release includes model weights and starting code for pre-trained and fine-tuned Llama language models, ranging from 7B to 70B parameters.

For more detailed examples leveraging Hugging Face, see [llama-recipes](https://github.com/facebookresearch/llama-recipes/).

### Updates Post-Launch

See [UPDATES.md](UPDATES.md) for the latest updates. For frequently asked questions, visit [here](https://ai.meta.com/llama/faq/).

### Download

To download model weights and tokenizer, visit the [Meta website](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) and accept our License. You will receive a signed URL over email. Use this URL with the `download.sh` script.

**Pre-requisites:** Ensure you have `wget` and `md5sum` installed. Run:
```bash
./download.sh
```
**Note:** Links expire after 24 hours. If you encounter a `403: Forbidden` error, re-request a link.

#### Access on Hugging Face

Download models from [Hugging Face](https://huggingface.co/meta-llama) by acknowledging the license and filling out the form in the model card of a repo.

### Quick Start

To run quick inference locally:
1. Clone and download this repository in a conda environment with PyTorch/CUDA.
2. Install:
    ```bash
    pip install -e .
    ```
3. Register and download the model from the [Meta website](https://ai.meta.com/resources/models-and-libraries/llama-downloads/).
4. Run the download script:
    ```bash
    ./download.sh
    ```
5. Execute the model locally:
    ```bash
    torchrun --nproc_per_node 1 example_chat_completion.py \
        --ckpt_dir llama-2-7b-chat/ \
        --tokenizer_path tokenizer.model \
        --max_seq_len 512 --max_batch_size 6
    ```

**Note:**
- Replace paths with your checkpoint directory and tokenizer model.
- Adjust `–nproc_per_node`, `max_seq_len`, and `max_batch_size` as needed.

### Inference

Model-parallel (MP) values:
|  Model | MP |
|--------|----|
| 7B     | 1  |
| 13B    | 2  |
| 70B    | 8  |

Models support sequence lengths up to 4096 tokens. Set `max_seq_len` and `max_batch_size` based on your hardware.

### Limitations

Llama models may sometimes generate factually incorrect or misleading responses. These inaccuracies are due to the model's reliance on learned patterns rather than real-time information.

**How to Address Factual Errors:**
- **Search and Verify:** Always verify critical information generated by the model. Cross-check facts with reliable sources.
- **Consider Multiple Sources:** Consult multiple references to ensure accuracy.
- **Safety Mitigations:** Use tools like [PurpleLlama](https://github.com/meta-llama/PurpleLlama) to implement safety checks and filters.

### Issues

Report software bugs or problems with the models through:
- [Model issues](http://github.com/facebookresearch/llama)
- [Risky content](http://developers.facebook.com/llama_output_feedback)
- [Bugs and security concerns](http://facebook.com/whitehat/info)

### Model Card

See [MODEL_CARD.md](MODEL_CARD.md).

### License

Our models and weights are licensed for both researchers and commercial entities. See the [LICENSE](LICENSE) file and [Acceptable Use Policy](USE_POLICY.md).

### References

1. [Research Paper](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)
2. [Llama 2 technical overview](https://ai.meta.com/resources/models-and-libraries/llama)
3. [Open Innovation AI Research Community](https://ai.meta.com/llama/open-innovation-ai-research-community/)

For common questions, see the [FAQ](https://ai.meta.com/llama/faq/).

## Original Llama

The repository for the original Llama release is in the [`llama_v1`](https://github.com/facebookresearch/llama/tree/llama_v1) branch.
